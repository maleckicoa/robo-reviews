{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTeMGaocHN68JBS+Nl+eeK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kr5red/automated-customer-reviews/blob/main/Model3_Ki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1)"
      ],
      "metadata": {
        "id": "_T7Qj-J_EBdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWTvXS3ZDA-c"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate torch sentencepiece bitsandbytes tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"reviews_with_sentiment.csv\")   # adjust filename\n",
        "PRODUCT_COL = \"name\"\n",
        "\n",
        "def light_clean(s): return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "df[\"text_for_sum\"] = df[\"text_merged\"].fillna(\"\").apply(light_clean)\n",
        "df_prod = (\n",
        "    df.groupby(PRODUCT_COL)[\"text_for_sum\"]\n",
        "      .apply(lambda xs: \"\\n\\n\".join(xs.tolist()))\n",
        "      .reset_index()\n",
        "      .rename(columns={\"text_for_sum\":\"all_reviews\"})\n",
        ")\n",
        "\n",
        "tqdm.pandas()\n",
        "df_prod[\"review_summary\"] = df_prod[\"all_reviews\"].progress_apply(lambda t: mistral_summarize(t))\n",
        "\n",
        "df_prod.to_csv(\"product_review_summaries_mistral_colab.csv\", index=False)"
      ],
      "metadata": {
        "id": "Duu4GyZMDJFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# 4-bit quantized loading â†’ fits in Colab GPU memory\n",
        "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "1Pn_gcx7DD-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mistral_summarize(text, max_new_tokens=220, temperature=0.3):\n",
        "    system = (\"You are a helpful assistant that writes coherent, fair product review summaries. \"\n",
        "              \"Write a single paragraph that naturally blends positive and negative opinions.\")\n",
        "    user = f\"Summarize the following customer reviews:\\n\\n{text}\"\n",
        "    prompt = f\"<s>[INST] <<SYS>>\\n{system}\\n<</SYS>>\\n\\n{user} [/INST]\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature, top_p=0.9)\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # keep only model reply after [/INST]\n",
        "    return result.split(\"[/INST]\")[-1].strip()"
      ],
      "metadata": {
        "id": "us9qlKeoDGxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}